{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN7mVXxSzh58CFhHAvpKjxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielGriseli/PLN_FALE_UFMG/blob/main/3/3_modelos_de_linguagem_e_n_gramas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos de Linguagem e N-gramas\n",
        "\n",
        "Ano passado eu morri, mas este ano eu não _____.\n",
        "\n",
        "*   vivo\n",
        "*   morro\n",
        "*   como\n",
        "*   corro\n",
        "\n",
        "Dado o contexto, qual a palavra mais provável?\n",
        "\n",
        "Resposta: **morro** - Sujeito de Sorte - Belchior (1976)\n",
        "\n",
        "---\n",
        "\n",
        "Eu no supermercado de bicicleta vou.\n",
        "\n",
        "**vs.**\n",
        "\n",
        "**Eu vou de bicicleta no supermercado.**\n",
        "\n",
        "Qual a sentença mais provável?\n",
        "\n",
        "---\n",
        "\n",
        "Poderoso você se tornou, o lado negro eu sinto em você.\n",
        "\n",
        "**vs.**\n",
        "\n",
        "Você se tornou poderoso, eu sinto o lado negro em você.\n",
        "\n",
        "Qual a sentença mais provável?\n",
        "\n",
        "Mestre Yoda escolheira a primeira :-)\n",
        "\n",
        "---\n",
        "\n",
        "Ouviram do Ipiranga as margens plácidas de um poco heróico o brado retumbante.\n",
        "\n",
        "**vs.**\n",
        "\n",
        "As margens plácidas do Ipiranga ouviram um brado retumbante de um povo heróico.\n",
        "\n",
        "Qual a sentença mais provável?"
      ],
      "metadata": {
        "id": "vcnwenBp-qAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questão\n",
        "\n",
        "Como ensinar um computador a definir as chances de uma palavra ou texto?"
      ],
      "metadata": {
        "id": "e5hUQr9ZB4R6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cálculo de Probabilidades\n",
        "\n",
        "Em Processamento de Língua Natural, um computador pode definir as chances de uma palavra ou texto através do **cálculo de probabilidades**:\n",
        "\n",
        "$$ P(w_{5} | w_{1}, w_{2}, w_{3}, w_{4}) $$\n",
        "\n",
        "Qual a probabilidade de uma palavra $ w_{5} $ dada a sequência de palavras $ w_{1}, w_{2}, w_{3}, w_{4} $?\n",
        "\n",
        "$$ P(w_{1}, w_{2}, w_{3}, w_{4}, w_{5}) $$\n",
        "\n",
        "Qual a probabilidade de um texto composto pela sequência de palavras $ w_{1}, w_{2}, w_{3}, w_{4}, w_{5} $?"
      ],
      "metadata": {
        "id": "KCuvKG59CEgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilidade da Próxima Palavra\n",
        "\n",
        "**Aplicações de geração de texto** produzem palavra por palavra a partir de suas probabilidades com base na sequência já gerada:\n",
        "\n",
        "| **Sequência**                            | **Próxima** | **Prob.** |\n",
        "|------------------------------------------|-------------|-----------|\n",
        "| Minha terra tem palmeiras, Onde canta o  | Pássaro     | 0,001     |\n",
        "|                                          | Homem       | 0,000001  |\n",
        "|                                          | **Sabiá**   | **0,031** |\n",
        "|                                          | Lobo        | 0,00001   |\n",
        "\n",
        "Probabilidade de 4 candidatas a próxima palavra de uma sequência de texto. **Sabiá** é a palavra inferida com probabilidade de 0,031, maior entre as 4 candidatas."
      ],
      "metadata": {
        "id": "KticyvwSDL5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilidade da Sentença\n",
        "\n",
        "Aplicações como **Máquinas de Tradução** definem a melhor tradução para uma sentença com base na probebilidade de cada tradução candidata.\n",
        "\n",
        "| **Sentença** | **Tradução**           | **Probabilidade** |\n",
        "|--------------|------------------------|-------------------|\n",
        "| 我在UFMG学习 | **Eu estudo na UFMG.** | **0.005**         |\n",
        "|              | Eu na UFMG estudo.     | 0.00002           |\n",
        "|              | Eu trabalho na UFMG.   | 0.0003            |\n",
        "\n",
        "Sentença em Chinês e as traduções candidatas em Português juntamente com suas probabilidades.\n",
        "\n",
        "Aplicações para correção de erros ortográficos também podem ser baseadas na probabilidade de sentenças:\n",
        "\n",
        "|                                      |   |                                     |\n",
        "|--------------------------------------|---|-------------------------------------|\n",
        "| **Texto Original**                   |   | Te encontro em 5 minutus.           |\n",
        "| **Opções pela Distância de Edição:** |   | minutos e minutas                   |\n",
        "| P(te, encontro, em, 5, **minutos**)  | > | P(te, encontro, em, 5, **minutas**) |"
      ],
      "metadata": {
        "id": "EyT98ytcEo2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de Linguagem\n",
        "\n",
        "Nome dado para modelos computacionais que inferem a probabilidade de uma sequência de palavras.\n",
        "\n",
        "Utilizado para a próxima palavra dada uma sequência\n",
        "\n",
        "$$ P(w_{5} | w_{1}, w_{2}, w_{3}, w_{4}) $$\n",
        "\n",
        "Utilizado para preves a ocorrência de uma sequência de palavras (sentença, texto, etc.)\n",
        "\n",
        "$$ P(w_{1}, w_{2}, w_{3}, w_{4}, w_{5}) $$"
      ],
      "metadata": {
        "id": "tBT3lkDNH5Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regra da Cadeia\n",
        "\n",
        "**Definição:** A probabilidade de uma sentença pode ser estimada através da regra da cadeia, i.e., a multiplicação das probabilidades de cada palavra, estimada com base nas palavras anteriores:\n",
        "\n",
        "$$ P(w_{1}, w_{2}, w_{3}, w_{4}) = P(w_{1}) \\times P(w_{2} | w_{1}) \\times P(w_{3} | w_{1}, w_{2}) \\times P(w_{4} | w_{1}, w_{2}, w_{3}) $$\n",
        "\n",
        "**Exemplo:**\n",
        "\n",
        "$$ \\mathbf{P(Minha \\, terra \\, tem \\, palmeiras) = P(Minha) \\times P(terra | Minha) \\times P(tem | Minha, terra) \\times P(palmeiras | Minha, terra, tem)} $$"
      ],
      "metadata": {
        "id": "cmkUZB5vIVcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimando Probabilidades\n",
        "\n",
        "Baseado num **corpus com muitas sentenças**, podemos estimar as probabilidades gazendo a contagem das sequências.\n",
        "\n",
        "**Exemplo:**\n",
        "\n",
        "$$ P(palmeiras | Minha, terra, tem)=\\frac{contar(Minha, terra, tem, palmeiras)}{contar(Minha, terra, tem)} $$\n",
        "\n",
        "**Problema:**\n",
        "\n",
        "Por maior que seja o córpus utilizado, muitas sequências longas não serão encontradas, resultando numa **probabilidade de valor 0**. Isso não é bom..."
      ],
      "metadata": {
        "id": "ZScQiBtnJwKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Propriedades de Markov\n",
        "\n",
        "**História:**\n",
        "\n",
        "Em 1913, Andrei Markov utilizou a **cadeia de Markov** para prever se, dada uma sequência do livro \"Eugene Onegin\", de Alexandre Pushkin, o próximo caractere era uma vogal ou consoante.\n",
        "\n",
        "**Cadeia de Markov:**\n",
        "\n",
        "Popularmente chamada de **N-gramas**, assume que a próxima palavra pode ser prevista com um pequeno conjunto de palavras prévias da sequência:\n",
        "\n",
        "$$ \\mathbf{P(<S> \\, Minha \\, terra \\, tem \\, palmeiras) = P(Minha | <S>) \\times P(terra | Minha) \\times P(tem | \\widetilde{Minha}, terra) \\times P(palmeiras | \\widetilde{Minha, terra}, tem)} $$"
      ],
      "metadata": {
        "id": "NJs8esatLfxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gramas\n",
        "\n",
        "\n",
        "\n",
        "*   Comumente, o cálculo de probabilidade da próxima palavra é feito com **bigramas** (sequência de 2 palavras) ou **trigramas** (sequência de 3 palavras)\n",
        "*   Contudo, 4gramas, 5gramas, Ngramas podem ser utilizados.\n",
        "*   Marcadores de início e fim de sentença devem ser utilizados.\n",
        "\n",
        "**Bigrama:**\n",
        "\n",
        "$$ P(palmeiras | tem)=\\frac{contar(tem, palmeiras)}{contar(tem)} $$\n",
        "\n",
        "**Trigrama:**\n",
        "\n",
        "$$ P(Minha | <S>, <S>)=\\frac{contar(<S>, <S>, Minha)}{contar(<S>, <S>)} $$\n"
      ],
      "metadata": {
        "id": "SqXpi6oXBN3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desafio\n",
        "\n",
        "no meio do caminho tinha uma pedra\n",
        "\n",
        "tinha uma pedra no meio do caminho\n",
        "\n",
        "tinha uma pedra\n",
        "\n",
        "no meio do caminho tinha uma pedra\n",
        "\n",
        "Quantos **unigramas** existem no trecho do poema \"No Meio do Caminho\" de Carlos Drummond de Andrade ao lado?\n",
        "\n",
        "*   no\n",
        "*   meio\n",
        "*   do\n",
        "*   caminho\n",
        "*   tinha\n",
        "*   uma\n",
        "*   pedra\n",
        "\n",
        "**Resposta: 7**\n",
        "\n",
        "Qual a probabilidade dos unigramas abaixo no trecho do poema ao lado?\n",
        "\n",
        "| **Unigrama** | **Cálculo** | **Resultado** |\n",
        "|--------------|-------------|---------------|\n",
        "| P(no)        | 3 ÷ 24      | 0,125         |\n",
        "| P(meio)      | 3 ÷ 24      | 0,125         |\n",
        "| P(do)        | 3 ÷ 24      | 0,125         |\n",
        "| P(caminho)   | 3 ÷ 24      | 0,125         |\n",
        "| P(tinha)     | 4 ÷ 24      | 0,17          |\n",
        "| P(uma)       | 4 ÷ 24      | 0,17          |\n",
        "| P(pedra)     | 4 ÷ 24      | 0,17          |\n",
        "\n",
        "Quantos **bigramas** existem no trecho do poema \"No Meio do Caminho\" de Carlos Drummond de Andrade ao lado?\n",
        "\n",
        "(\\<s>, no), (no, meio), (meio, do), (do, caminho), (caminho, tinha), (tinha, uma), (uma, pedra), (pedra, \\</s>), (\\<s>, tinha), (pedra, no), (caminho, \\</s>)\n",
        "\n",
        "**Resposta: 11**\n",
        "\n",
        "Qual a probabilidade dos **bigramas** abaixo no trecho do poema?\n",
        "\n",
        "| **Bigrama**         | **Cálculo** | **Resultado** |\n",
        "|---------------------|-------------|---------------|\n",
        "| P(no \\| \\<s>)        | 2 ÷ 4       | 0,5           |\n",
        "| P(meio \\| no)       | 3 ÷ 3       | 1             |\n",
        "| P(do \\| meio)       | 3 ÷ 3       | 1             |\n",
        "| P(caminho \\| do)    | 3 ÷ 3       | 1             |\n",
        "| P(tinha \\| caminho) | 2 ÷ 3       | 0,67          |\n",
        "| P(uma \\| tinha)     | 4 ÷ 4       | 1             |\n",
        "| P(pedra \\| uma)     | 4 ÷ 4       | 1             |\n",
        "| P(\\</s> \\| pedra)    | 3 ÷ 4       | 0,75          |\n",
        "| P(tinha \\| \\<s>)     | 2 ÷ 4       | 0,5           |\n",
        "| P(no \\| pedra)      | 1 ÷ 4       | 0,25          |\n",
        "| P(\\</s> \\| caminho)  | 1 ÷ 3       | 0,34          |\n",
        "\n",
        "De acordo com os bigramas computados, qual a probabilidade do verso abaixo?\n",
        "\n",
        "\\<s>tinha uma pedra \\</s>\n",
        "\n",
        "$$ P(<s>, tinha, uma, pedra, </s>) = $$\n",
        "$$ P(tinha | <s>) \\times P(uma | tinha) \\times P(pedra | uma) \\times P(</s> | pedra) = $$\n",
        "$$ 0,5 \\times 1 \\times 1 \\times 0,75 = $$\n",
        "$$ \\mathbf{0,375} $$"
      ],
      "metadata": {
        "id": "PZuYdollWDVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dica\n",
        "\n",
        "*   A medida que o número de palavras cresce numa sequência, sua probabilidade tende a zero.\n",
        "*   Para evitar extrapolar o número de casas decimais de uma variável flutuante em Python, probabilidades podem ser calculadas pela soma de seus lagaritmos.\n",
        "\n",
        "$$ P(inicio, w_{1}, w_{2}, fim) = P(w_{1} | inicio) \\times P(w_{2} | w_{1}) \\times P(fim | w_{2}) $$\n",
        "\n",
        "$$ = log(P(w_{1} | inicio)) + log(P(w_{2} | P(w_{1})) + log(P(fim | P(w_{2})) $$"
      ],
      "metadata": {
        "id": "bpAFg1mDDjV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementação em Python\n",
        "\n",
        "\n",
        "\n",
        "1.   Definir e carregar um córpus de texto para cálculo das probabilidades\n",
        "    * Aula 10.1: Leitura e Escrita de Arquivos (Curso de *Python*)\n",
        "2.   Tokenizar as sentenças\n",
        "    * Aula 2: Segmentação e Padronização de Textos (Curso de PLN)\n",
        "3.   Inserir os marcadores de Início e fim de sentença\n",
        "4.   Calcular os n-gramas (e.g., bigramas, etc) utilizando o NLTK\n",
        "5.   Colocar todos os tokens do córpus numa única lista (*flatten*)\n",
        "6.   Definir o Vocabulário\n",
        "7.   Treinar um modelo de linguagem\n",
        "\n"
      ],
      "metadata": {
        "id": "yCwDNjU5miiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixando a versão 3.5 do NLTK"
      ],
      "metadata": {
        "id": "JQsRzsR2oLar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install nltk==3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eClzEWZooQWw",
        "outputId": "70a5523a-9d42-4d68-f3da-35a70497a913"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (1.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (4.66.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=8db01b7123c37081c99f4805fd2533e3406ddb3eff05985136ee0abc779726f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/ab/82/f9667f6f884d272670a15382599a9c753a1dfdc83f7412e37d\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando Dependências"
      ],
      "metadata": {
        "id": "OXozQv72ocRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t-n_YS6ogj7",
        "outputId": "3809e2d2-5105-40b4-9d0f-19a4cfc57a0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 1: Carregando o Córpus"
      ],
      "metadata": {
        "id": "6VqpWuBQomN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"\"\"No meio do caminho tinha uma pedra\n",
        "Tinha uma pedra no meio do caminho\n",
        "Tinha uma pedra\n",
        "No meio do caminho tinha uma pedra\"\"\"\n",
        "\n",
        "texto = texto.lower().split('\\n')\n",
        "texto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K9nWpS2o8Gj",
        "outputId": "83ecc0eb-f241-4f55-d2d7-110b7261ff3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['no meio do caminho tinha uma pedra',\n",
              " 'tinha uma pedra no meio do caminho',\n",
              " 'tinha uma pedra',\n",
              " 'no meio do caminho tinha uma pedra']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 2: Tokenizando as Sentenças do Córpus\n",
        "\n",
        "(Lembrem-se da Aula 2:Segmentação e Padronização de Textos)"
      ],
      "metadata": {
        "id": "aNGygHI8o7dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_tok = []\n",
        "for verso in texto:\n",
        "  tokens = nltk.word_tokenize(verso, language='portuguese')\n",
        "  texto_tok.append(tokens)\n",
        "\n",
        "texto_tok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY5tzO5apOQb",
        "outputId": "7d9cf64b-45bb-4805-a355-0cc78e8fa057"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra'],\n",
              " ['tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho'],\n",
              " ['tinha', 'uma', 'pedra'],\n",
              " ['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pré-processando as Sentenças"
      ],
      "metadata": {
        "id": "ttvd4hOMplry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 3: Inserindo Marcadores de Início e Fim de Sentença\n",
        "\n",
        "Suponha que queiramos definir um modelo de linguagem com bigramas, ou seja, calcular as chances de uma palavra com base na anterior (e.g., $ P(Pedra | uma) $, temos que marcar o início e fim da sentença para poder prever as chances da primeira palavra (e.g., $ P(no | <s>) $) e o fim da sentença (e.g., $ P(</s> | pedra) $. Este processo é conhecido como *padding*.\n",
        "\n",
        "Podemos fazer o *padding* de uma sentença utilizando o método **nltk.lm.preprocessing.pad_both_ends**:"
      ],
      "metadata": {
        "id": "pu7Sqy4qrBtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "\n",
        "ngramas = 2\n",
        "\n",
        "texto_tok_pad = []\n",
        "for verso in texto_tok:\n",
        "  padded = list(pad_both_ends(verso, n=ngramas))\n",
        "  texto_tok_pad.append(padded)\n",
        "\n",
        "texto_tok_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ4rvwqsq-tT",
        "outputId": "30590ca5-6857-40c1-9462-f90753f6cdc5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<s>', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', '</s>'],\n",
              " ['<s>', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', '</s>'],\n",
              " ['<s>', 'tinha', 'uma', 'pedra', '</s>'],\n",
              " ['<s>', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', '</s>']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 4: Calculando os N-Gramas\n",
        "\n",
        "Uma vez que as sentenças do nosso córpus foram pré-processadas, podemos calcular os n-gramas (neste caso, os bigramas), utilizando o método **nltk.ngrams**:"
      ],
      "metadata": {
        "id": "6KZT2b2Jrdkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngramas = 2\n",
        "\n",
        "bigramas_pad = []\n",
        "for verso in texto_tok_pad:\n",
        "  bigramas = list(nltk.ngrams(verso, n=ngramas))\n",
        "  bigramas_pad.append(bigramas)\n",
        "\n",
        "bigramas_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY3V2dnjr5Mo",
        "outputId": "7078a453-8540-4177-e509-b733c9c0c07b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')],\n",
              " [('<s>', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', '</s>')],\n",
              " [('<s>', 'tinha'), ('tinha', 'uma'), ('uma', 'pedra'), ('pedra', '</s>')],\n",
              " [('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contudo, para deixar nosso modelo de linguagem mais robusto, vamos calcular os **unigramas** além dos **bigramas** utilizando o comando **nltk.util.everygrams**:"
      ],
      "metadata": {
        "id": "rXTfKjmsr4SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import everygrams\n",
        "\n",
        "ngramas = 2\n",
        "ngramas_pad = []\n",
        "for verso in texto_tok_pad:\n",
        "  bigramas = list(everygrams(verso, max_len=ngramas))\n",
        "  ngramas_pad.append(bigramas)\n",
        "\n",
        "ngramas_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwUEP-cjsYmL",
        "outputId": "b871bb84-9536-4774-c414-0b69d8a79c98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('<s>',),\n",
              "  ('no',),\n",
              "  ('meio',),\n",
              "  ('do',),\n",
              "  ('caminho',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')],\n",
              " [('<s>',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('no',),\n",
              "  ('meio',),\n",
              "  ('do',),\n",
              "  ('caminho',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', '</s>')],\n",
              " [('<s>',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')],\n",
              " [('<s>',),\n",
              "  ('no',),\n",
              "  ('meio',),\n",
              "  ('do',),\n",
              "  ('caminho',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 5: Colocando todos os tokens do córpus numa única lista\n",
        "\n",
        "**nltk.lm.preprocessing.flatten**:\n",
        "\n",
        "Este método converte junta os elementos de sublistas em uma única lista. Por exemplo:\n",
        "\n",
        "```\n",
        ">>> lista = [[1, 2], [3, 4]]\n",
        ">>> flatten(lista)\n",
        "[1, 2, 3, 4]\n",
        "```\n",
        "\n",
        "Como pode ser visto abaixo, nós o utilizamos para juntar todos os tokens das sentenças de nosso corpus numa única lista."
      ],
      "metadata": {
        "id": "4mbz9HO4uzwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import flatten\n",
        "\n",
        "tokens = list(flatten(texto_tok_pad))\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNZotw-IvHZI",
        "outputId": "316c8fbb-a951-4b0f-a67b-051889af835e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'no',\n",
              " 'meio',\n",
              " 'do',\n",
              " 'caminho',\n",
              " 'tinha',\n",
              " 'uma',\n",
              " 'pedra',\n",
              " '</s>',\n",
              " '<s>',\n",
              " 'tinha',\n",
              " 'uma',\n",
              " 'pedra',\n",
              " 'no',\n",
              " 'meio',\n",
              " 'do',\n",
              " 'caminho',\n",
              " '</s>',\n",
              " '<s>',\n",
              " 'tinha',\n",
              " 'uma',\n",
              " 'pedra',\n",
              " '</s>',\n",
              " '<s>',\n",
              " 'no',\n",
              " 'meio',\n",
              " 'do',\n",
              " 'caminho',\n",
              " 'tinha',\n",
              " 'uma',\n",
              " 'pedra',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Passo 6: Definindo o Vocabulário\n",
        "\n",
        "**nltk.lm.Vocabulary**\n",
        "\n",
        "Utilizado para definir o vocabulário do nosso córpus. Recebe dois parâmetros como entrada: uma lista com todos os tokens do nosso córpus e a variável *unk_cutoff*, a qual passa a considerar palavras abaixo de um limiar de frequência como palavras fora do vocabulário."
      ],
      "metadata": {
        "id": "Qq0CnTVdviYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import Vocabulary\n",
        "\n",
        "vocab = Vocabulary(tokens, unk_cutoff=1)"
      ],
      "metadata": {
        "id": "o_RE6ojBwoJD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtendo as frequências das palavras do córpus com o comando *counts*"
      ],
      "metadata": {
        "id": "sXiQ4gwEwu4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcSUR83Hw1FT",
        "outputId": "030e4120-68e5-4c6b-f89d-0f57690cdd67"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'<s>': 4,\n",
              "         'no': 3,\n",
              "         'meio': 3,\n",
              "         'do': 3,\n",
              "         'caminho': 3,\n",
              "         'tinha': 4,\n",
              "         'uma': 4,\n",
              "         'pedra': 4,\n",
              "         '</s>': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "procurando uma palavra no vocabulário. Caso não encontrada, o token de palavra fora do vocabulário será retornado (\\<UNK>)"
      ],
      "metadata": {
        "id": "egj_FpMWxCnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.lookup(\"rocha\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "blT9gZjLxOzh",
        "outputId": "fbc9e582-9868-4203-8dba-58ac399c4152"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<UNK>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simplificando o Pré-processamento\n",
        "\n",
        "Agora que voc~e sabe cada passo do pré-processamento (inserir marcadores de início e fim de sentença, calcular os n-gramas, juntar todos os tokens do corpus numa lista e definir o vocabulário(, este processo pode ser simplificado pela funcionalidade **nltk.lm.preprocessing.padded_everygram_pipeline**:"
      ],
      "metadata": {
        "id": "guITJjT5xSne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "ngramas = 2\n",
        "\n",
        "ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)"
      ],
      "metadata": {
        "id": "cCvWRHWXx0LN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 7: Treinando um modelo de linguagem\n",
        "\n",
        "Um modelo de linguagem pode ser treinado utilizando a funcionalidade **nltk.lm.MLE**"
      ],
      "metadata": {
        "id": "A0tC6gEZyh0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import MLE\n",
        "\n",
        "ngramas = 2\n",
        "lm = MLE(ngramas)\n",
        "lm.fit(ngramas_pad, vocab)"
      ],
      "metadata": {
        "id": "_xILmfh9yvLK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado o token **\\<s>**, gerando um texto de 4 tokens com o modelo de linguagem treinado."
      ],
      "metadata": {
        "id": "ZMFYah46y-N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm.generate(4, text_seed=[\"<s>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vd5tlLOzIb_",
        "outputId": "2a63be90-1a86-40f4-c1e8-48f3c0f322fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['no', 'meio', 'do', 'caminho']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probabilidade da palavra *no*:"
      ],
      "metadata": {
        "id": "LyY0hvbCzRXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm.score(\"no\"), lm.logscore(\"no\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAf9sB2IzWt9",
        "outputId": "490efc01-34d8-4092-b7c5-72ea9b24ccd7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.09375, -3.415037499278844)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probabilidade da palavra *tinha* dado a palavra *caminho*:"
      ],
      "metadata": {
        "id": "puVP5C-VzcRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm.score(\"tinha\", context=[\"caminho\"]), lm.logscore(\"tinha\", context=[\"caminho\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOJxwSKmzhxk",
        "outputId": "f457ea12-00bc-4618-a80b-2ed72fbda10c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, -0.5849625007211563)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avaliação\n",
        "\n",
        "*   Como saber se um modelo de linguagem é bom?\n",
        "*   Ou seja, como saber que as probabilidades geradas refletem as chances de um cenário real?\n",
        "\n",
        "**Resposta:** Cálculo da Perrlexidade\n",
        "\n",
        "$$ PP(W) = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(W_{i}|W_{1}...W_{i-1})}} $$\n",
        "$$ =(\\prod_{i=1}^{N}\\frac{1}{P(W_{i}|W_{1}...W_{i-1})})^{-\\frac{1}{N}} $$\n",
        "\n",
        "Quanto menor a perplexidade, melhor o modelo."
      ],
      "metadata": {
        "id": "pYsL6Co47WI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Avaliação: Perplexidade\n",
        "\n",
        "\n",
        "\n",
        "1.   Definir um **córpus de treino** e um **córpus de teste**\n",
        "2.   Pré-processar os dois corpus\n",
        "    1. Tokenizar as sentenças, inserir os marcadores de início e fim, calcular os n-gramas\n",
        "3.   Com o córpus pré-processado de treino:\n",
        "    1. Definir o vacabulário\n",
        "    2. Treinar o Modelo de Linguagem\n",
        "4.   Com o córpus pré-processado de teste:\n",
        "    1. Calcular a prerplexidade no modelo de linguagem treinado\n",
        "\n",
        "*   **Nunca calcule a perplexidade com o conjunto de treinamento.**\n",
        "*   **A avaliação será enviesada**"
      ],
      "metadata": {
        "id": "t9ZtXVHuAcyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teste = \"\"\"Tinha uma pedra\n",
        "No meio do caminho\n",
        "Tinha uma pedra\"\"\"\n",
        "\n",
        "teste = teste.lower().split('\\n')\n",
        "teste_tok = []\n",
        "for verso in teste:\n",
        "  tokens = nltk.word_tokenize(verso, language='portuguese')\n",
        "  teste_tok.append(tokens)\n",
        "\n",
        "ngramas = 1\n",
        "teste_ngramas, _ = padded_everygram_pipeline(ngramas, teste_tok)\n",
        "teste_ngramas = flatten([list(w) for w in teste_ngramas])\n",
        "print('Unigramas: ', lm.perplexity(teste_ngramas))\n",
        "\n",
        "ngramas = 2\n",
        "teste_ngramas, _ = padded_everygram_pipeline(ngramas, teste_tok)\n",
        "teste_ngramas = flatten([list(w) for w in teste_ngramas])\n",
        "print('Bigramas: ', lm.perplexity(teste_ngramas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3fOibXDCAzw",
        "outputId": "b5364ef1-ba95-4398-8b51-0e8a66a4c6d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigramas:  8.975641163569597\n",
            "Bigramas:  3.7299192471355798\n"
          ]
        }
      ]
    }
  ]
}